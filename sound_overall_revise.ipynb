{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSipd9B1KACN",
        "outputId": "220de52e-b995-42b2-bb76-94d7a2a02244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: praat-parselmouth in /usr/local/lib/python3.10/dist-packages (0.4.5)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from praat-parselmouth) (1.26.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install praat-parselmouth\n",
        "# 필요한 라이브러리 설치\n",
        "!pip install pydub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "# M4A 파일을 WAV로 변환\n",
        "audio = AudioSegment.from_file(\"real_test.m4a\", format=\"m4a\")\n",
        "audio.export(\"real_test.wav\", format=\"wav\")\n",
        "audio2 = AudioSegment.from_file(\"seoul_test4.mp3\", format=\"mp3\")\n",
        "audio2.export(\"seoul_test4.wav\", format=\"wav\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "S5NDbBDgKiE3",
        "outputId": "35f7ff51-846a-4d74-fae8-b3887126b1e8"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'real_test.m4a'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-c169be4df9ce>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# M4A 파일을 WAV로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"real_test.m4a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"m4a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"real_test.wav\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"wav\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0maudio2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAudioSegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"seoul_test4.mp3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mp3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydub/audio_segment.py\u001b[0m in \u001b[0;36mfrom_file\u001b[0;34m(cls, file, format, codec, parameters, start_second, duration, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fd_or_path_or_tempfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pydub/utils.py\u001b[0m in \u001b[0;36m_fd_or_path_or_tempfile\u001b[0;34m(fd, mode, tempfile)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mclose_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'real_test.m4a'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "음성 분석"
      ],
      "metadata": {
        "id": "p889UHFFKqyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "인간 음성 분석"
      ],
      "metadata": {
        "id": "J1U0GvrwKsZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#인간의 음역대만 따와서 필터링한다. 여유시간을 둬서 완성도 있게 필터링함.\n",
        "import parselmouth\n",
        "import scipy.io.wavfile as wav\n",
        "import numpy as np\n",
        "\n",
        "def process_and_save_filtered_audio(input_file_path, output_file_path, human_voice_range=(30, 255), extend_ms=50, silence_after_ms=150):\n",
        "    \"\"\"\n",
        "    입력 WAV 파일에서 인간 음역대만 필터링하여 결과를 반환하고, 새 파일로 저장하는 함수.\n",
        "\n",
        "    Args:\n",
        "        input_file_path (str): 입력 WAV 파일 경로.\n",
        "        output_file_path (str): 출력 필터링된 WAV 파일 경로.\n",
        "        human_voice_range (tuple): 인간 음역대 (최소 Hz, 최대 Hz).\n",
        "        extend_ms (int): 유지할 길이 (밀리초).\n",
        "        silence_after_ms (int): 여유 시간 (밀리초).\n",
        "\n",
        "    Returns:\n",
        "        filtered_data (numpy.ndarray): 필터링된 오디오 데이터.\n",
        "        pitch_values (numpy.ndarray): 피치 값 배열.\n",
        "        time_steps (numpy.ndarray): 시간 단계 배열.\n",
        "    \"\"\"\n",
        "    # 1. WAV 파일 로드\n",
        "    sampling_rate, data = wav.read(input_file_path)\n",
        "\n",
        "    # 스테레오일 경우 단일 채널로 변환\n",
        "    if len(data.shape) > 1:\n",
        "        data = data.mean(axis=1).astype(np.int16)\n",
        "\n",
        "    # 2. parselmouth로 음성 분석\n",
        "    snd = parselmouth.Sound(data, sampling_rate)\n",
        "\n",
        "    # 피치 분석\n",
        "    pitch = snd.to_pitch()\n",
        "\n",
        "    # 피치 값과 해당 시간 단계 가져오기\n",
        "    pitch_values = pitch.selected_array[\"frequency\"]\n",
        "    time_steps = pitch.xs()\n",
        "\n",
        "    # 3. 히스테리시스 필터 적용\n",
        "    filtered_data = np.zeros_like(data, dtype=np.float32)\n",
        "    prev_in_range = False\n",
        "\n",
        "    # 샘플 수로 변환\n",
        "    extend_samples = int(sampling_rate * extend_ms / 1000)\n",
        "    silence_samples = int(sampling_rate * silence_after_ms / 1000)\n",
        "\n",
        "    for i, t in enumerate(time_steps):\n",
        "        index = int(t * sampling_rate)\n",
        "\n",
        "        # 현재 피치가 인간 음역대에 해당하는지 확인\n",
        "        in_range = human_voice_range[0] <= pitch_values[i] <= human_voice_range[1]\n",
        "\n",
        "        if in_range or prev_in_range:\n",
        "            # 앞뒤로 extend_ms만큼 구간을 유지\n",
        "            start = max(0, index - extend_samples)\n",
        "            end = min(len(data), index + extend_samples + silence_samples)\n",
        "            filtered_data[start:end] = data[start:end]\n",
        "\n",
        "        prev_in_range = in_range\n",
        "\n",
        "    # 4. 결과 WAV 파일 저장\n",
        "    wav.write(output_file_path, sampling_rate, filtered_data.astype(np.int16))\n",
        "\n",
        "    print(f\"{output_file_path} 파일이 생성되었습니다.\")\n",
        "    return sampling_rate,filtered_data, pitch_values, time_steps\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J3CaIjvdKnNY"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 사용\n",
        "sampling_rate, filtered_data, pitch_values, time_steps = process_and_save_filtered_audio(\n",
        "    input_file_path=\"real_test.wav\",\n",
        "    output_file_path=\"real_test_only.wav\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlCMf0XbKuF5",
        "outputId": "cf1985e3-9773-40e2-d8bf-23c985a3b5df"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "real_test_only.wav 파일이 생성되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tts 음성 분석"
      ],
      "metadata": {
        "id": "MF2oHdkuKxGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import parselmouth\n",
        "import scipy.io.wavfile as wav\n",
        "import numpy as np\n",
        "\n",
        "def extract_pitch_from_tts(file_path):\n",
        "    \"\"\"\n",
        "    TTS 음성을 분석하여 피치 값을 추출하는 함수.\n",
        "\n",
        "    매개변수:\n",
        "    file_path (str): TTS 음성 파일 경로 (WAV 형식).\n",
        "\n",
        "    반환값:\n",
        "    tuple: (샘플링 레이트, 피치 값 배열, 시간 단계 배열)\n",
        "    \"\"\"\n",
        "    # 1. WAV 파일 로드\n",
        "    tts_sampling_rate, tts_data = wav.read(file_path)\n",
        "\n",
        "    # 스테레오일 경우 단일 채널로 변환\n",
        "    if len(tts_data.shape) > 1:\n",
        "        tts_data = tts_data.mean(axis=1).astype(np.int16)\n",
        "\n",
        "    # 2. parselmouth로 음성 분석\n",
        "    snd = parselmouth.Sound(tts_data, tts_sampling_rate)\n",
        "\n",
        "    # 피치 분석\n",
        "    pitch = snd.to_pitch()\n",
        "\n",
        "    # 피치 값과 해당 시간 단계 가져오기\n",
        "    pitch_values_tts = pitch.selected_array[\"frequency\"]\n",
        "    time_steps_tts = pitch.xs()\n",
        "\n",
        "    # 피치 값과 시간 단계 배열 반환\n",
        "    return tts_sampling_rate, tts_data, pitch_values_tts, time_steps_tts\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U-b7lHpzKyh1"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 사용\n",
        "file_path = \"seoul_test4.wav\"  # 분석할 TTS 음성 파일 경로\n",
        "tts_sampling_rate, tts_data, pitch_values_tts, time_steps_tts = extract_pitch_from_tts(file_path)"
      ],
      "metadata": {
        "id": "MiNMLk4QK1Kf"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "타임스탬프"
      ],
      "metadata": {
        "id": "c69nF_TKK28L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/m-bain/whisperx.git\n",
        "!pip install ffmpeg\n",
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX1QYOPEK6CZ",
        "outputId": "8150c56d-83d7-468f-f053-9803ab915bb8"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/m-bain/whisperx.git\n",
            "  Cloning https://github.com/m-bain/whisperx.git to /tmp/pip-req-build-8sbx0nzm\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/m-bain/whisperx.git /tmp/pip-req-build-8sbx0nzm\n",
            "  Resolved https://github.com/m-bain/whisperx.git to commit 9e3a9e0e38fcec1304e1784381059a0e2c670be5\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (2.5.0+cu121)\n",
            "Requirement already satisfied: torchaudio>=2 in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (2.5.0+cu121)\n",
            "Requirement already satisfied: faster-whisper==1.0.0 in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (1.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (4.39.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (2.2.2)\n",
            "Requirement already satisfied: setuptools>=65 in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (75.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (3.8.1)\n",
            "Requirement already satisfied: pyannote.audio==3.1.1 in /usr/local/lib/python3.10/dist-packages (from whisperx==3.1.1) (3.1.1)\n",
            "Requirement already satisfied: av==11.* in /usr/local/lib/python3.10/dist-packages (from faster-whisper==1.0.0->whisperx==3.1.1) (11.0.0)\n",
            "Requirement already satisfied: ctranslate2<5,>=4.0 in /usr/local/lib/python3.10/dist-packages (from faster-whisper==1.0.0->whisperx==3.1.1) (4.5.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper==1.0.0->whisperx==3.1.1) (0.24.7)\n",
            "Requirement already satisfied: tokenizers<0.16,>=0.13 in /usr/local/lib/python3.10/dist-packages (from faster-whisper==1.0.0->whisperx==3.1.1) (0.15.2)\n",
            "Requirement already satisfied: onnxruntime<2,>=1.14 in /usr/local/lib/python3.10/dist-packages (from faster-whisper==1.0.0->whisperx==3.1.1) (1.20.0)\n",
            "Requirement already satisfied: asteroid-filterbanks>=0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (0.4.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (0.8.0)\n",
            "Requirement already satisfied: lightning>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (2.4.0)\n",
            "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (2.3.0)\n",
            "Requirement already satisfied: pyannote.core>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (5.0.0)\n",
            "Requirement already satisfied: pyannote.database>=5.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (5.1.0)\n",
            "Requirement already satisfied: pyannote.metrics>=3.2 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (3.2.1)\n",
            "Requirement already satisfied: pyannote.pipeline>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (3.0.1)\n",
            "Requirement already satisfied: pytorch-metric-learning>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (2.7.0)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (13.9.4)\n",
            "Requirement already satisfied: semver>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (3.0.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (0.12.1)\n",
            "Requirement already satisfied: speechbrain>=0.5.14 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (1.0.2)\n",
            "Requirement already satisfied: tensorboardX>=2.6 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (2.6.2.2)\n",
            "Requirement already satisfied: torch-audiomentations>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (0.11.1)\n",
            "Requirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio==3.1.1->whisperx==3.1.1) (1.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->whisperx==3.1.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2->whisperx==3.1.1) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.1) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->whisperx==3.1.1) (4.66.6)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas->whisperx==3.1.1) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->whisperx==3.1.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->whisperx==3.1.1) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->whisperx==3.1.1) (2024.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->whisperx==3.1.1) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->whisperx==3.1.1) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->whisperx==3.1.1) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->whisperx==3.1.1) (0.4.5)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (0.11.8)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (2.4.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio==3.1.1->whisperx==3.1.1) (4.9.3)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper==1.0.0->whisperx==3.1.1) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper==1.0.0->whisperx==3.1.1) (24.3.25)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime<2,>=1.14->faster-whisper==1.0.0->whisperx==3.1.1) (3.20.3)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio==3.1.1->whisperx==3.1.1) (2.4.0)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio==3.1.1->whisperx==3.1.1) (1.13.1)\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (0.12.5)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (1.5.2)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (0.6.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (3.8.0)\n",
            "Requirement already satisfied: optuna>=3.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (4.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->whisperx==3.1.1) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio==3.1.1->whisperx==3.1.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio==3.1.1->whisperx==3.1.1) (2.18.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.17.1)\n",
            "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.10/dist-packages (from speechbrain>=0.5.14->pyannote.audio==3.1.1->whisperx==3.1.1) (1.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain>=0.5.14->pyannote.audio==3.1.1->whisperx==3.1.1) (0.2.0)\n",
            "Requirement already satisfied: julius<0.3,>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.2.7)\n",
            "Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.10.2.post1)\n",
            "Requirement already satisfied: torch-pitch-shift>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (1.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->whisperx==3.1.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->whisperx==3.1.1) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->whisperx==3.1.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->whisperx==3.1.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->whisperx==3.1.1) (2024.8.30)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio==3.1.1->whisperx==3.1.1) (2.22)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (3.10.10)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (3.0.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.60.0)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (1.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (3.2.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.14.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (2.0.36)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio==3.1.1->whisperx==3.1.1) (3.5.0)\n",
            "Requirement already satisfied: primePy>=1.3 in /usr/local/lib/python3.10/dist-packages (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (1.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.5.4)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.10/dist-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper==1.0.0->whisperx==3.1.1) (10.0)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.10/dist-packages (from hyperpyyaml->speechbrain>=0.5.14->pyannote.audio==3.1.1->whisperx==3.1.1) (0.18.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (4.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (1.3.6)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio==3.1.1->whisperx==3.1.1) (4.3.6)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=0.5.14->pyannote.audio==3.1.1->whisperx==3.1.1) (0.2.12)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (3.1.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio==3.1.1->whisperx==3.1.1) (0.2.0)\n",
            "Requirement already satisfied: ffmpeg in /usr/local/lib/python3.10/dist-packages (1.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "타임스탬프를 구하는 함수"
      ],
      "metadata": {
        "id": "Y5C7Dj1hK-kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisperx\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "def extract_word_timestamps(audio_file, model_size=\"medium\", device='cpu', batch_size=16, compute_type=\"int8\"):\n",
        "\n",
        "    #print(\"Is CUDA available:\", torch.cuda.is_available())\n",
        "    #print(\"Number of CUDA devices:\", torch.cuda.device_count())\n",
        "\n",
        "    # 1. Whisper 모델 로드 및 음성 텍스트 변환\n",
        "    model = whisperx.load_model(model_size, device, compute_type=compute_type, language = \"ko\")\n",
        "\n",
        "    # 오디오 파일 로드\n",
        "    audio = whisperx.load_audio(audio_file)\n",
        "    result = model.transcribe(audio, batch_size=batch_size, language=\"ko\")\n",
        "\n",
        "    # 2. Whisper 출력 정렬\n",
        "    model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
        "    result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
        "\n",
        "    # 3. 단어별 타임스탬프 리스트 생성\n",
        "    timestamps = []\n",
        "    for segment in result[\"segments\"]:\n",
        "        for word in segment[\"words\"]:\n",
        "            timestamps.append({\n",
        "                \"word\": word[\"word\"],\n",
        "                \"start\": word[\"start\"],\n",
        "                \"end\": word[\"end\"]\n",
        "            })\n",
        "\n",
        "    return timestamps\n",
        "\n"
      ],
      "metadata": {
        "id": "ZRaaok4kK8Fj"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "사용자 타임스탬프"
      ],
      "metadata": {
        "id": "pKRgDjNMLAtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 예시 사용\n",
        "audio_file_path = \"real_test_only.wav\"\n",
        "word_intervals = extract_word_timestamps(audio_file_path, model_size=\"medium\")\n",
        "\n",
        "# word_intervals에서 start와 end 값을 0.01씩 조정\n",
        "for i, item in enumerate(word_intervals):\n",
        "    # 첫 번째 단어의 start는 변경하지 않음\n",
        "    if i != 0 and item['start'] >= 0.01:\n",
        "        item['start'] = round(item['start'] - 0.01, 3)\n",
        "    item['end'] = round(item['end'] + 0.01, 3)\n",
        "\n",
        "# 결과 출력\n",
        "for item in word_intervals:\n",
        "    print(f\"단어: {item['word']}, 시작 시간: {item['start']}, 끝 시간: {item['end']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VQhP3paLDSX",
        "outputId": "899ab268-257b-438f-cd61-d281637306ac"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.5.0+cu121. Bad things might happen unless you revert torch to 1.x.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at kresnik/wav2vec2-large-xlsr-korean were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at kresnik/wav2vec2-large-xlsr-korean and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어: 안녕하세요., 시작 시간: 0.401, 끝 시간: 1.073\n",
            "단어: 저는, 시작 시간: 1.294, 끝 시간: 1.474\n",
            "단어: 지금, 시작 시간: 1.555, 끝 시간: 1.775\n",
            "단어: 서울, 시작 시간: 1.855, 끝 시간: 2.056\n",
            "단어: 말을, 시작 시간: 2.116, 끝 시간: 2.297\n",
            "단어: 구사하는, 시작 시간: 2.377, 끝 시간: 2.838\n",
            "단어: 중입니다., 시작 시간: 2.939, 끝 시간: 3.44\n",
            "단어: 이거, 시작 시간: 4.022, 끝 시간: 4.223\n",
            "단어: 어디까지, 시작 시간: 4.243, 끝 시간: 4.664\n",
            "단어: 올라가는, 시작 시간: 4.704, 끝 시간: 5.125\n",
            "단어: 거예요?, 시작 시간: 5.186, 끝 시간: 6.008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "tts 타임스탬프"
      ],
      "metadata": {
        "id": "aatpjg7rLD3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 예시 사용\n",
        "audio_file_path = \"seoul_test4.wav\"\n",
        "tts_word_intervals = extract_word_timestamps(audio_file_path, model_size=\"medium\")\n",
        "\n",
        "# 결과 출력\n",
        "for item in tts_word_intervals:\n",
        "    print(f\"단어: {item['word']}, 시작 시간: {item['start']}, 끝 시간: {item['end']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYS83xaPLEy_",
        "outputId": "b66420f2-4263-415b-9378-7e067d827116"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model was trained with pyannote.audio 0.0.1, yours is 3.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
            "Model was trained with torch 1.10.0+cu102, yours is 2.5.0+cu121. Bad things might happen unless you revert torch to 1.x.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at kresnik/wav2vec2-large-xlsr-korean were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
            "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at kresnik/wav2vec2-large-xlsr-korean and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어: 안녕하세요., 시작 시간: 0.009, 끝 시간: 0.732\n",
            "단어: 저는, 시작 시간: 1.054, 끝 시간: 1.335\n",
            "단어: 지금, 시작 시간: 1.476, 끝 시간: 1.737\n",
            "단어: 서울, 시작 시간: 1.897, 끝 시간: 2.058\n",
            "단어: 말을, 시작 시간: 2.159, 끝 시간: 2.36\n",
            "단어: 구사하는, 시작 시간: 2.48, 끝 시간: 2.862\n",
            "단어: 중입니다., 시작 시간: 2.942, 끝 시간: 3.384\n",
            "단어: 이거, 시작 시간: 4.027, 끝 시간: 4.288\n",
            "단어: 어디까지, 시작 시간: 4.369, 끝 시간: 4.911\n",
            "단어: 올라가는, 시작 시간: 4.971, 끝 시간: 5.373\n",
            "단어: 거예요?, 시작 시간: 5.453, 끝 시간: 6.237\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "음성 비교 하기"
      ],
      "metadata": {
        "id": "dEKcdQQgLGkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. 진폭차이로 구하기"
      ],
      "metadata": {
        "id": "MIucXI7PLJQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_amplitude_differences(user_word_intervals, tts_word_intervals, filtered_data, tts_data, threshold):\n",
        "    # 데이터 타입을 float으로 변환하여 오버플로우 방지\n",
        "    filtered_data = filtered_data.astype(np.float32)\n",
        "    tts_data = tts_data.astype(np.float32)\n",
        "    max_diff_word = None\n",
        "    max_amplitude_difference = 0  # 최고 진폭 차이 추적용\n",
        "    user_threshold_exceeding_words = []\n",
        "    tts_threshold_exceeding_words = []\n",
        "    for user_word, tts_word in zip(user_word_intervals, tts_word_intervals):\n",
        "        # 사용자 음성 단어 세그먼트의 시작과 끝 인덱스 계산\n",
        "        user_start_idx = int(user_word['start'] * sampling_rate)\n",
        "        user_end_idx = int(user_word['end'] * sampling_rate)\n",
        "\n",
        "        # TTS 음성 단어 세그먼트의 시작과 끝 인덱스 계산\n",
        "        tts_start_idx = int(tts_word['start'] * tts_sampling_rate)\n",
        "        tts_end_idx = int(tts_word['end'] * tts_sampling_rate)\n",
        "\n",
        "        # 사용자 음성의 진폭 값 중 100 이상인 값만 필터링\n",
        "        user_amplitude = filtered_data[user_start_idx:user_end_idx]\n",
        "        user_amplitude = user_amplitude[user_amplitude >= 100]\n",
        "\n",
        "        # TTS 음성의 진폭 값 중 100 이상인 값만 필터링\n",
        "        tts_amplitude = tts_data[tts_start_idx:tts_end_idx]\n",
        "        tts_amplitude = tts_amplitude[tts_amplitude >= 100]\n",
        "\n",
        "        # 사용자와 TTS 음성의 최대 및 최소 진폭 계산 (필터링된 값만 사용)\n",
        "        if len(user_amplitude) > 0 and len(tts_amplitude) > 0:\n",
        "            user_max_amplitude = np.max(user_amplitude)\n",
        "            user_min_amplitude = np.min(user_amplitude)\n",
        "            user_amplitude_difference = user_max_amplitude - user_min_amplitude\n",
        "\n",
        "            tts_max_amplitude = np.max(tts_amplitude)\n",
        "            tts_min_amplitude = np.min(tts_amplitude)\n",
        "            tts_amplitude_difference = tts_max_amplitude - tts_min_amplitude\n",
        "\n",
        "            max_amplitude_difference_temp = user_max_amplitude - tts_max_amplitude\n",
        "\n",
        "            if max_amplitude_difference < max_amplitude_difference_temp :\n",
        "              max_amplitude_difference = max_amplitude_difference_temp\n",
        "              max_diff_word = {\"word\": user_word['word']}\n",
        "            if user_amplitude_difference - tts_amplitude_difference > threshold :\n",
        "              user_threshold_exceeding_words.append({\"word\": user_word['word']})\n",
        "\n",
        "            elif tts_amplitude_difference - user_amplitude_difference > threshold:\n",
        "              tts_threshold_exceeding_words.append({\"word\": user_word['word']})\n",
        "\n",
        "            '''\n",
        "            # 진폭 차이 비교\n",
        "            amplitude_diff = abs(user_amplitude_difference - tts_amplitude_difference)\n",
        "            if amplitude_diff > threshold:\n",
        "                print(f\"단어: {user_word['word']}, 사용자 진폭 차이: {user_amplitude_difference}, TTS 진폭 차이: {tts_amplitude_difference}, 차이: {amplitude_diff}\")\n",
        "\n",
        "              '''\n",
        "\n",
        "\n",
        "    return user_threshold_exceeding_words,tts_threshold_exceeding_words,max_diff_word\n"
      ],
      "metadata": {
        "id": "KQvLRSbLLKmV"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예제 사용\n",
        "threshold_value = 5600  # 임계값 설정\n",
        "user_exceeding_words, tts_exceeding_words, max_word = compare_amplitude_differences(word_intervals, tts_word_intervals, filtered_data, tts_data, threshold_value)\n",
        "\n",
        "for word_info in user_exceeding_words:\n",
        "    print(f\"단어: {word_info['word']}이(가) 표준어 대비 세기 차이가 강해요\")\n",
        "\n",
        "for word_info in tts_exceeding_words:\n",
        "    print(f\"단어: {word_info['word']}이(가) 표준어 대비 세기 차이가 약해요\")\n",
        "\n",
        "print(f\"단어: {max_word['word']}이(가) 너무 세게 말해요.\")\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c20YTZO3LVDK",
        "outputId": "7517ab65-02ff-4776-a527-e7bc71084b23"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어: 거예요?이(가) 표준어 대비 세기 차이가 강해요\n",
            "단어: 저는이(가) 표준어 대비 세기 차이가 약해요\n",
            "단어: 지금이(가) 표준어 대비 세기 차이가 약해요\n",
            "단어: 서울이(가) 표준어 대비 세기 차이가 약해요\n",
            "단어: 말을이(가) 표준어 대비 세기 차이가 약해요\n",
            "단어: 이거이(가) 표준어 대비 세기 차이가 약해요\n",
            "단어: 올라가는이(가) 표준어 대비 세기 차이가 약해요\n",
            "단어: 거예요?이(가) 너무 세게 말해요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 피치 차이로 구하기"
      ],
      "metadata": {
        "id": "wllnYuZ8LOOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_pitch_differences(word_intervals, tts_word_intervals, pitch_values, time_steps, pitch_values_tts, time_steps_tts, threshold=25):\n",
        "    \"\"\"\n",
        "    사용자와 TTS 단어의 피치 차이를 계산하고 결과를 반환하는 함수.\n",
        "\n",
        "    Args:\n",
        "        word_intervals (list): 사용자의 단어 간격 리스트.\n",
        "        tts_word_intervals (list): TTS 단어 간격 리스트.\n",
        "        pitch_values (np.array): 사용자 음성의 피치 값 배열.\n",
        "        time_steps (np.array): 사용자 음성의 타임스탬프 배열.\n",
        "        pitch_values_tts (np.array): TTS 음성의 피치 값 배열.\n",
        "        time_steps_tts (np.array): TTS 음성의 타임스탬프 배열.\n",
        "        threshold (int): 피치 차이 임계값.\n",
        "\n",
        "    Returns:\n",
        "        list: 피치 차이가 임계값을 초과하는 단어의 결과 리스트.\n",
        "    \"\"\"\n",
        "    user_results = []\n",
        "    tts_results = []\n",
        "    # 사용자 단어와 TTS 단어의 피치 차이 계산\n",
        "    for i in range(min(len(word_intervals), len(tts_word_intervals))):  # 단어 개수의 최소값으로 루프\n",
        "        user_word = word_intervals[i]  # 사용자의 단어\n",
        "        tts_word = tts_word_intervals[i]  # TTS 단어\n",
        "\n",
        "        # 사용자 피치 샘플링 추출 (0이 아닌 값만 선택)\n",
        "        user_pitch_samples = pitch_values[(time_steps >= user_word['start']) & (time_steps <= user_word['end'])]\n",
        "        user_time_samples = time_steps[(time_steps >= user_word['start']) & (time_steps <= user_word['end'])]\n",
        "        user_pitch_samples = user_pitch_samples[user_pitch_samples > 0]  # 0이 아닌 값만 포함\n",
        "\n",
        "        if len(user_pitch_samples) > 0:\n",
        "            user_pitch_max = np.max(user_pitch_samples)\n",
        "            user_pitch_min = np.min(user_pitch_samples)\n",
        "            user_pitch_diff = user_pitch_max - user_pitch_min\n",
        "            user_max_time = user_time_samples[np.argmax(user_pitch_samples)]  # 최대 피치에 해당하는 시간\n",
        "\n",
        "            # TTS 피치 샘플링 추출 (0이 아닌 값만 선택)\n",
        "            tts_pitch_samples = pitch_values_tts[(time_steps_tts >= tts_word['start']) & (time_steps_tts <= tts_word['end'])]\n",
        "            tts_time_samples = time_steps_tts[(time_steps_tts >= tts_word['start']) & (time_steps_tts <= tts_word['end'])]\n",
        "            tts_pitch_samples = tts_pitch_samples[tts_pitch_samples > 0]  # 0이 아닌 값만 포함\n",
        "\n",
        "            if len(tts_pitch_samples) > 0:\n",
        "                tts_pitch_max = np.max(tts_pitch_samples)\n",
        "                tts_pitch_min = np.min(tts_pitch_samples)\n",
        "                tts_pitch_diff = tts_pitch_max - tts_pitch_min\n",
        "                tts_max_time = tts_time_samples[np.argmax(tts_pitch_samples)]  # 최대 피치에 해당하는 시간\n",
        "                if user_pitch_diff - tts_pitch_diff > threshold:\n",
        "                  user_results.append({'word': user_word['word']})\n",
        "                if tts_pitch_diff - user_pitch_diff > threshold:\n",
        "                  tts_results.append({'word': user_word['word']})\n",
        "\n",
        "                '''\n",
        "                # 피치 차이가 임계값 초과하는 경우 결과 저장\n",
        "                if user_pitch_diff > threshold or tts_pitch_diff > threshold:\n",
        "                    results.append({\n",
        "                        'user_word': user_word['word'],\n",
        "                        'tts_word': tts_word['word'],\n",
        "                        'user_diff': user_pitch_diff,\n",
        "                        'user_max_pitch': user_pitch_max,\n",
        "                        'user_max_time': user_max_time,\n",
        "                        'user_min_pitch': user_pitch_min,\n",
        "                        'tts_diff': tts_pitch_diff,\n",
        "                        'tts_max_pitch': tts_pitch_max,\n",
        "                        'tts_max_time': tts_max_time,\n",
        "                        'tts_min_pitch': tts_pitch_min\n",
        "                    })\n",
        "\n",
        "                '''\n",
        "\n",
        "    return user_results,tts_results\n",
        "\n"
      ],
      "metadata": {
        "id": "JQPiMIaGLQd4"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 사용 예시\n",
        "u_results,t_results = calculate_pitch_differences(\n",
        "    word_intervals, tts_word_intervals, pitch_values, time_steps, pitch_values_tts, time_steps_tts\n",
        ")\n",
        "\n",
        "for word_info in u_results:\n",
        "    print(f\"단어: {word_info['word']}이(가) 표준어 대비 억양 차이가 심해요\")\n",
        "for word_info in t_results:\n",
        "    print(f\"단어: {word_info['word']}이(가) 표준어 대비 억양 차이가 약해요\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir4tTqBWLu0A",
        "outputId": "432fbe87-7cd0-470d-8a11-c0af28571d08"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "단어: 안녕하세요.이(가) 표준어 대비 억양 차이가 약해요\n",
            "단어: 구사하는이(가) 표준어 대비 억양 차이가 약해요\n",
            "단어: 어디까지이(가) 표준어 대비 억양 차이가 약해요\n",
            "단어: 거예요?이(가) 표준어 대비 억양 차이가 약해요\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 기울기 평균으로 구하기"
      ],
      "metadata": {
        "id": "kwsTu4lrLcZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Gradient 계산 함수\n",
        "def calculate_pitch_gradients(pitch_values, time_steps):\n",
        "    gradients = []\n",
        "\n",
        "    # 피치 기울기 계산\n",
        "    for i in range(1, len(pitch_values)):\n",
        "        if pitch_values[i] > 0 and pitch_values[i-1] > 0:  # 피치가 0보다 큰 경우에만 계산\n",
        "            gradient = (pitch_values[i] - pitch_values[i-1]) / (time_steps[i] - time_steps[i-1])\n",
        "            gradients.append(gradient)\n",
        "\n",
        "    return gradients\n",
        "\n",
        "# 세그먼트 기울기 비교 함수\n",
        "def compare_segments(user_segments, tts_segments, user_pitch_values, user_time_steps, tts_pitch_values, tts_time_steps):\n",
        "    highest_segment = None\n",
        "    lowest_segment = None\n",
        "    highest_difference = -float('inf')  # Initialize to negative infinity for maximum comparison\n",
        "    lowest_difference = float('inf')    # Initialize to positive infinity for minimum comparison\n",
        "\n",
        "    for user_segment, tts_segment in zip(user_segments, tts_segments):\n",
        "        # 해당 세그먼트의 피치 값 및 타임스탬프 추출\n",
        "        user_indices = [i for i, t in enumerate(user_time_steps) if user_segment['start'] <= t <= user_segment['end']]\n",
        "        tts_indices = [i for i, t in enumerate(tts_time_steps) if tts_segment['start'] <= t <= tts_segment['end']]\n",
        "\n",
        "        # 피치 기울기 계산\n",
        "        if user_indices and tts_indices:\n",
        "            user_segment_pitch = user_pitch_values[user_indices[0]:user_indices[-1] + 1]\n",
        "            user_segment_time = user_time_steps[user_indices[0]:user_indices[-1] + 1]\n",
        "            tts_segment_pitch = tts_pitch_values[tts_indices[0]:tts_indices[-1] + 1]\n",
        "            tts_segment_time = tts_time_steps[tts_indices[0]:tts_indices[-1] + 1]\n",
        "\n",
        "            user_gradients = calculate_pitch_gradients(user_segment_pitch, user_segment_time)\n",
        "            tts_gradients = calculate_pitch_gradients(tts_segment_pitch, tts_segment_time)\n",
        "\n",
        "            # 기울기 차이 계산\n",
        "            if user_gradients and tts_gradients:\n",
        "                avg_user_gradient = np.mean(user_gradients)\n",
        "                avg_tts_gradient = np.mean(tts_gradients)\n",
        "                difference = avg_user_gradient - avg_tts_gradient\n",
        "\n",
        "                # 가장 높은 기울기 차이를 기록 (양의 방향)\n",
        "                if difference > highest_difference:\n",
        "                    highest_difference = difference\n",
        "                    highest_segment = (user_segment, tts_segment, avg_user_gradient, avg_tts_gradient)\n",
        "\n",
        "                # 가장 낮은 기울기 차이를 기록 (음의 방향)\n",
        "                if difference < lowest_difference:\n",
        "                    lowest_difference = difference\n",
        "                    lowest_segment = (user_segment, tts_segment, avg_user_gradient, avg_tts_gradient)\n",
        "\n",
        "    return highest_segment, lowest_segment\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jQnP-_ZVLaxD"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 세그먼트 비교\n",
        "highest_segment, lowest_segment = compare_segments(\n",
        "    word_intervals, tts_word_intervals, pitch_values, time_steps, pitch_values_tts, time_steps_tts\n",
        ")\n",
        "\n",
        "# 결과 출력\n",
        "if highest_segment:\n",
        "    user_segment_high, tts_segment_high, avg_user_gradient_high, avg_tts_gradient_high = highest_segment\n",
        "    print(f\"음성이 TTS 대비 가장 높은 기울기를 가진 세그먼트:\")\n",
        "    print(f\"사용자 세그먼트: {user_segment_high}, TTS 세그먼트: {tts_segment_high}\")\n",
        "    print(f\"사용자 평균 기울기: {avg_user_gradient_high:.4f}, TTS 평균 기울기: {avg_tts_gradient_high:.4f}\")\n",
        "    print(\"음성이 TTS 대비 기울기가 높아요.\")\n",
        "\n",
        "if lowest_segment:\n",
        "    user_segment_low, tts_segment_low, avg_user_gradient_low, avg_tts_gradient_low = lowest_segment\n",
        "    print(f\"음성이 TTS 대비 가장 낮은 기울기를 가진 세그먼트:\")\n",
        "    print(f\"사용자 세그먼트: {user_segment_low}, TTS 세그먼트: {tts_segment_low}\")\n",
        "    print(f\"사용자 평균 기울기: {avg_user_gradient_low:.4f}, TTS 평균 기울기: {avg_tts_gradient_low:.4f}\")\n",
        "    print(\"음성이 TTS 대비 기울기가 낮아요.\")\n",
        "else:\n",
        "    print(\"세그먼트 비교에서 차이를 찾을 수 없습니다.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-bzpf_BL8d1",
        "outputId": "c9172e57-69ef-415d-fc6a-73d198d70f12"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "음성이 TTS 대비 가장 높은 기울기를 가진 세그먼트:\n",
            "사용자 세그먼트: {'word': '어디까지', 'start': 4.243, 'end': 4.664}, TTS 세그먼트: {'word': '어디까지', 'start': 4.369, 'end': 4.911}\n",
            "사용자 평균 기울기: -83.1606, TTS 평균 기울기: -179.3498\n",
            "음성이 TTS 대비 기울기가 높아요.\n",
            "음성이 TTS 대비 가장 낮은 기울기를 가진 세그먼트:\n",
            "사용자 세그먼트: {'word': '지금', 'start': 1.555, 'end': 1.775}, TTS 세그먼트: {'word': '지금', 'start': 1.476, 'end': 1.737}\n",
            "사용자 평균 기울기: -235.7869, TTS 평균 기울기: -22.2522\n",
            "음성이 TTS 대비 기울기가 낮아요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. DTW로 유사도 점수"
      ],
      "metadata": {
        "id": "XOiMzOhmMo_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastdtw"
      ],
      "metadata": {
        "id": "_qMXXmG4WPSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import librosa\n",
        "from scipy.spatial.distance import euclidean\n",
        "from fastdtw import fastdtw\n",
        "\n",
        "def extract_features(audio_file):\n",
        "    # Load audio file\n",
        "    y, sr = librosa.load(audio_file, sr=None)\n",
        "\n",
        "    # Extract MFCCs\n",
        "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "\n",
        "    # Extract pitch (fundamental frequency)\n",
        "    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)\n",
        "    pitch_values = pitches[pitches > 0]\n",
        "    if len(pitch_values) > 0:\n",
        "        pitch_mean = np.mean(pitch_values)  # Average pitch value\n",
        "    else:\n",
        "        pitch_mean = 0  # Handle cases with no pitch\n",
        "\n",
        "    return mfccs, pitch_mean\n",
        "\n",
        "def calculate_similarity(mfcc1, pitch1, mfcc2, pitch2):\n",
        "    # Calculate DTW distance between MFCCs\n",
        "    distance, _ = fastdtw(mfcc1.T, mfcc2.T, dist=euclidean)\n",
        "\n",
        "    # Normalize DTW distance to a similarity score\n",
        "    max_distance = max(mfcc1.shape[1], mfcc2.shape[1]) * np.max([np.max(np.abs(mfcc1)), np.max(np.abs(mfcc2))])\n",
        "    dtw_similarity = max(0, 1 - (distance / (max_distance + 1e-9)))  # Add small constant to avoid division by zero\n",
        "\n",
        "    # Calculate pitch difference as a similarity\n",
        "    pitch_diff = abs(pitch1 - pitch2)\n",
        "    max_pitch_range = 400  # Adjust this range if necessary for normalization\n",
        "    normalized_pitch_diff = max(0, 1 - (pitch_diff / max_pitch_range))\n",
        "\n",
        "    # Combine similarities with weighted averaging\n",
        "    similarity_score = 0.7 * dtw_similarity + 0.3 * normalized_pitch_diff\n",
        "\n",
        "    # Scale similarity score to a 0-100 range\n",
        "    return similarity_score * 100\n",
        "\n",
        "def evaluate_score(score):\n",
        "    if score >= 90:\n",
        "        return \"Excellent! You're singing almost perfectly.\"\n",
        "    elif score >= 75:\n",
        "        return \"Good job! You're very close.\"\n",
        "    elif score >= 50:\n",
        "        return \"Not bad! Keep practicing.\"\n",
        "    elif score >= 30:\n",
        "        return \"Needs improvement. Try again!\"\n",
        "    else:\n",
        "        return \"Keep practicing! Don't give up!\"\n",
        "\n",
        "def main(user_audio_file, tts_audio_file):\n",
        "    # Extract features from both audio files\n",
        "    mfcc_user, pitch_user = extract_features(user_audio_file)\n",
        "    mfcc_tts, pitch_tts = extract_features(tts_audio_file)\n",
        "\n",
        "    # Calculate similarity score\n",
        "    similarity_score = calculate_similarity(mfcc_user, pitch_user, mfcc_tts, pitch_tts)\n",
        "\n",
        "    # Ensure the score is capped between 0 and 100\n",
        "    similarity_score = min(max(similarity_score, 0), 100)\n",
        "\n",
        "    # Evaluate the score\n",
        "    evaluation = evaluate_score(similarity_score)\n",
        "\n",
        "    print(f\"Similarity Score: {similarity_score:.2f}/100\")\n",
        "    print(evaluation)\n",
        "\n"
      ],
      "metadata": {
        "id": "cJAWtPgZMq65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 예시 사용법\n",
        "user_audio_file = 'real_test_only.wav'  # 사용자 음성 파일\n",
        "tts_audio_file = 'seoul_test4.wav'  # TTS 음성 파일\n",
        "\n",
        "main(user_audio_file, tts_audio_file)\n"
      ],
      "metadata": {
        "id": "kMSbjnzsMtCR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}